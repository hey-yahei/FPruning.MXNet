{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MXNet package\n",
    "from mxnet import nd, init, cpu, gpu, gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon.data.vision import CIFAR10, transforms as T\n",
    "from gluoncv.model_zoo import cifar_resnet56_v1\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "\n",
    "# Normal package\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom package\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from prune import PrunerManager, WeightL1RankPruner\n",
    "import presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # Model\n",
    "    num_class = 10\n",
    "    \n",
    "    # Train\n",
    "    max_steps = 15000\n",
    "    train_batch_size = 128\n",
    "    val_batch_size = 256\n",
    "    train_num_workers = 4\n",
    "    val_num_workers = 4\n",
    "    lr = 1e-5\n",
    "    lr_decay = []\n",
    "    lr_decay_at = []\n",
    "    \n",
    "    # Record\n",
    "    ckpt_dir = \"./checkpoints\"\n",
    "    main_tag = 'cifar10_resnet56_v1_reg'\n",
    "    ckpt_prefix = 'cifa10_resnet_56_v1_reg'\n",
    "    train_record_per_steps = 200\n",
    "    val_per_steps = 400\n",
    "    spotter_starts_at = 10000\n",
    "    spotter_window_size = 10\n",
    "    patience = 20\n",
    "    snapshot_per_steps = 400\n",
    "    \n",
    "    # Prune(fixed std, with regularization)\n",
    "    enable_reg = True\n",
    "    reg_lambda = 0.1\n",
    "    prune_at = [0, 1200, 2400, 3600, 4800, 6000, 7200]\n",
    "    prune_std = [.4,] * 7\n",
    "    \n",
    "#     # Prune(increasing std, w/o regularization)\n",
    "#     enable_reg = False\n",
    "#     reg_lambda = 0.1\n",
    "#     prune_at = [0, 1200, 2400, 3600, 4800, 6000, 7200]\n",
    "#     prune_std = [.4, .45, .5, .55, .6, .65, .7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(Config.ckpt_dir):\n",
    "    os.mkdir(Config.ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_stamp = time.strftime('%Y%m%d_%H%M%S',time.localtime(time.time()))\n",
    "writer = SummaryWriter(log_dir=\"runs/{}_{}\".format(Config.main_tag, datetime_stamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cifar_resnet56_v1(pretrained=True)\n",
    "net.collect_params().reset_ctx(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs, bns, shares = presets.cifar10_resnet56_v1(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = PrunerManager(net)\n",
    "for conv, bn in zip(convs, bns):\n",
    "    manager.add(\n",
    "        WeightL1RankPruner(conv, bn, shares.get(conv, None))\n",
    "    )\n",
    "manager.build((1,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, num_class, dataloader, ctx):\n",
    "    t = time.time()\n",
    "    correct_counter = nd.zeros(num_class)\n",
    "    label_counter = nd.zeros(num_class)\n",
    "    test_num_correct = 0\n",
    "    eval_loss = 0.\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "\n",
    "        outputs = net(X)\n",
    "        loss = loss_func(outputs, y)\n",
    "        eval_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        test_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "\n",
    "        pred = pred.as_in_context(cpu())\n",
    "        y = y.astype('float32').as_in_context(cpu())\n",
    "        for p, gt in zip(pred, y):\n",
    "            label_counter[gt] += 1\n",
    "            if p == gt:\n",
    "                correct_counter[gt] += 1\n",
    "\n",
    "    eval_loss /= len(test_dataset)\n",
    "    eval_acc = test_num_correct / len(test_dataset)\n",
    "    eval_acc_avg = (correct_counter / (label_counter+1e-10)).mean().asscalar()\n",
    "    \n",
    "    return eval_loss, eval_acc, eval_acc_avg, time.time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/cifar/train_cifar10.py#L97\n",
    "transform_train = T.Compose([\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "    T.RandomFlipLeftRight(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(train=True).transform_first(transform_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=Config.train_batch_size, shuffle=True, \n",
    "                          num_workers=Config.train_num_workers, last_batch='discard')\n",
    "test_dataset = CIFAR10(train=False).transform_first(transform_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=Config.val_batch_size, shuffle=False, \n",
    "                          num_workers=Config.val_num_workers, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(test_dataset)\n",
    "print(f'trainset size => {train_size}')\n",
    "print(f'valset size => {val_size}')\n",
    "steps_per_epoch = train_size / Config.train_batch_size\n",
    "print(f'{steps_per_epoch} steps for per epoch (BATCH_SIZE={Config.train_batch_size})')\n",
    "print(\"record per {} steps ({} samples, {} times per epoch)\".format(\n",
    "                                                            Config.train_record_per_steps,\n",
    "                                                            Config.train_record_per_steps * Config.train_batch_size,\n",
    "                                                            steps_per_epoch / Config.train_record_per_steps))\n",
    "print(\"evaluate per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.val_per_steps,\n",
    "                                                    steps_per_epoch / Config.val_per_steps))\n",
    "print(\"spotter start at {} steps ({} epoches)\".format(\n",
    "                                                Config.spotter_starts_at,\n",
    "                                                Config.spotter_starts_at / steps_per_epoch))\n",
    "print(\"size of spotter window is {} ({} steps)\".format(\n",
    "                                                Config.spotter_window_size,\n",
    "                                                Config.spotter_window_size * Config.val_per_steps))\n",
    "print(\"max patience: {} ({} steps; {} samples; {} epoches)\".format(\n",
    "                                                            Config.patience,\n",
    "                                                            Config.patience * Config.val_per_steps,\n",
    "                                                            Config.patience * Config.val_per_steps * Config.train_batch_size,\n",
    "                                                            Config.patience * Config.val_per_steps / steps_per_epoch))\n",
    "print(\"snapshot per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.snapshot_per_steps,\n",
    "                                                    steps_per_epoch / Config.snapshot_per_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "good_acc_window = [0.] * Config.spotter_window_size\n",
    "estop_loss_window = [0.] * Config.patience\n",
    "quantize_input_offline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': Config.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_per_record = Config.train_record_per_steps * Config.train_batch_size\n",
    "flag_early_stop = False\n",
    "train_loss = 0.\n",
    "train_num_correct = 0\n",
    "t = time.time()\n",
    "\n",
    "# First evaluate\n",
    "eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, 0)\n",
    "writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "    'val': eval_acc,\n",
    "    'val_avg': eval_acc_avg\n",
    "}, 0)\n",
    "# First prune\n",
    "if Config.prune_at and Config.prune_at[0] == 0:\n",
    "    Config.prune_at.pop(0)\n",
    "    s = Config.prune_std.pop(0)\n",
    "    manager.prune(s)\n",
    "    # evaluate again\n",
    "    eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "    writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, 1)\n",
    "    writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "        'val': eval_acc,\n",
    "        'val_avg': eval_acc_avg\n",
    "    }, 1)\n",
    "    pparams, pmac = manager.analyse()\n",
    "    writer.add_scalars(f'{Config.main_tag}/pruned', {\"params\": pparams, \"mac\": pmac}, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while global_steps < Config.max_steps and not flag_early_stop:\n",
    "    for X, y in train_loader:\n",
    "        # Move data to gpu\n",
    "        X = X.as_in_context(gpu(0))\n",
    "        y = y.as_in_context(gpu(0))\n",
    "        # Forward & Backward\n",
    "        with autograd.record():\n",
    "            outputs = net(X)\n",
    "            if Config.enable_reg and Config.prune_at:\n",
    "                loss = loss_func(outputs, y) + Config.reg_lambda * manager.group_lasso()    # regularization\n",
    "            else:   # fixed prune_ratio, fine-tune without regularization\n",
    "                loss = loss_func(outputs, y)\n",
    "        loss.backward()\n",
    "        trainer.step(Config.train_batch_size)\n",
    "        \n",
    "        train_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        train_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "        \n",
    "        # Record training info\n",
    "        if global_steps and global_steps % Config.train_record_per_steps == 0:\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'train': train_loss/size_per_record}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {'train': train_num_correct/size_per_record}, global_steps)\n",
    "            train_loss = 0.\n",
    "            train_num_correct = 0\n",
    "            \n",
    "        # Evaluate\n",
    "        if global_steps and global_steps % Config.val_per_steps == 0:\n",
    "            # Prune\n",
    "            if Config.prune_at and global_steps >= Config.prune_at[0]:\n",
    "                Config.prune_at.pop(0)\n",
    "                s = Config.prune_std.pop(0)\n",
    "                manager.prune(s)\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "            writer.add_scalar(f'{Config.main_tag}/Speed', Config.val_per_steps / (time.time() - t), global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "                'val': eval_acc,\n",
    "                'val_avg': eval_acc_avg\n",
    "            }, global_steps)\n",
    "            pparams, pmac = manager.analyse()\n",
    "            writer.add_scalars(f'{Config.main_tag}/pruned', {\"params\": pparams, \"mac\": pmac}, global_steps)\n",
    "            \n",
    "#             # Spotter\n",
    "#             good_acc_window.pop(0)\n",
    "#             if global_steps >= Config.spotter_starts_at and eval_acc > max(good_acc_window):\n",
    "#                 print( \"catch a good model with acc {:.6f} at {} step\".format(eval_acc, global_steps) )\n",
    "#                 writer.add_text(Config.main_tag, \"catch a good model with acc {:.6f}\".format(eval_acc), global_steps)\n",
    "#                 net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))\n",
    "#             good_acc_window.append(eval_acc)\n",
    "\n",
    "#             # Early stop\n",
    "#             estop_loss_window.pop(0)\n",
    "#             estop_loss_window.append(eval_loss)\n",
    "#             if global_steps > Config.val_per_steps * len(estop_loss_window):\n",
    "#                 min_index = estop_loss_window.index( min(estop_loss_window) )\n",
    "#                 writer.add_scalar(f'{Config.main_tag}/val/Patience', min_index, global_steps)\n",
    "#                 if min_index == 0:\n",
    "#                     flag_early_stop = True\n",
    "#                     print(\"early stop at {} steps\".format(global_steps))\n",
    "#                     break\n",
    "\n",
    "            # lr decay\n",
    "            if Config.lr_decay_at and global_steps >= Config.lr_decay_at[0]:\n",
    "                new_lr = trainer.learning_rate * Config.lr_decay[0]\n",
    "                print(f\"[{global_steps}]update lr: {trainer.learning_rate} -> {new_lr}\")\n",
    "                trainer.set_learning_rate(new_lr)\n",
    "                Config.lr_decay.pop(0)\n",
    "                Config.lr_decay_at.pop(0)\n",
    "            \n",
    "            t = time.time()\n",
    "        \n",
    "        # Snapshot\n",
    "        if global_steps and global_steps % Config.snapshot_per_steps == 0:\n",
    "            net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))\n",
    "            \n",
    "        # Next step\n",
    "        global_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
